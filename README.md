## Large (Vision) Language Models are Unsupervised In-Context Learners

[Artyom Gadetsky*](http://agadetsky.github.io), [Andrei Atanov*](http://andrewatanov.github.io/), [Yulun Jiang*](https://yljblues.github.io), [Zhitong Gao](https://gaozhitong.github.io/), [Ghazal Hosseini Mighan](https://www.linkedin.com/in/ghazal-hosseini-mighan-8b911823a/), [Amir Zamir](https://vilab.epfl.ch/zamir/), [Maria BrbiÄ‡](https://brbiclab.epfl.ch/team/)

[`Project page`](https://brbiclab.epfl.ch/projects/joint-inference/) | [`Paper`](https://openreview.net/pdf?id=ohJxgRLlLt) | [`BibTeX`](#citing) 
_________________
<div align="justify">
This repo contains the source code of the joint inference framework, a framework for large (vision) language models to perform unsupervised adaptation on a given task, resulting in the improved performance upon independent zero-shot predictions. Our framework is compatible with any existing large (vision) language model and, remarkably, despite being fully unsupervised, it often performs on par with supervised approaches that rely on ground truth labels. For more details please check our paper <a href="https://openreview.net/pdf?id=ohJxgRLlLt">Large (Vision) Language Models are Unsupervised In-Context Learners</a> (ICLR '25).
</div>


### Citing

If you find our code useful, please consider citing:

```
@inproceedings{gadetsky2025large,
    title={Large (Vision) Language Models are Unsupervised In-Context Learners},
    author={Artyom Gadetsky and Andrei Atanov and Yulun Jiang and Zhitong Gao and Ghazal Hosseini Mighan and Amir Zamir and Maria Brbic},
    booktitle={International Conference on Learning Representations},
    year={2025},
}
```
